/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().
  warnings.warn(_BETA_TRANSFORMS_WARNING)
Not init distributed mode.
Start training
Load PResNet50 state_dict
Initial lr:  [1e-05, 0.0001, 0.0001, 0.0001]
loading annotations into memory...
Done (t=0.12s)
creating index...
index created!
loading annotations into memory...
Done (t=0.12s)
creating index...
index created!
number of params: 42700515
Epoch: [0]  [  0/315]  eta: 0:16:40  lr: 0.000010  loss: 42.6095 (42.6095)  loss_vfl: 0.0106 (0.0106)  loss_bbox: 1.3869 (1.3869)  loss_giou: 2.7831 (2.7831)  loss_vfl_aux_0: 0.0093 (0.0093)  loss_bbox_aux_0: 1.4022 (1.4022)  loss_giou_aux_0: 2.8081 (2.8081)  loss_vfl_aux_1: 0.0093 (0.0093)  loss_bbox_aux_1: 1.3750 (1.3750)  loss_giou_aux_1: 2.7843 (2.7843)  loss_vfl_aux_2: 0.0105 (0.0105)  loss_bbox_aux_2: 1.3788 (1.3788)  loss_giou_aux_2: 2.7798 (2.7798)  loss_vfl_aux_3: 0.0100 (0.0100)  loss_bbox_aux_3: 1.3929 (1.3929)  loss_giou_aux_3: 2.7637 (2.7637)  loss_vfl_aux_4: 0.0092 (0.0092)  loss_bbox_aux_4: 1.3814 (1.3814)  loss_giou_aux_4: 2.8017 (2.8017)  loss_vfl_aux_5: 0.0092 (0.0092)  loss_bbox_aux_5: 1.3707 (1.3707)  loss_giou_aux_5: 2.7676 (2.7676)  loss_vfl_dn_0: 0.7953 (0.7953)  loss_bbox_dn_0: 0.1027 (0.1027)  loss_giou_dn_0: 1.4077 (1.4077)  loss_vfl_dn_1: 0.7052 (0.7052)  loss_bbox_dn_1: 0.1027 (0.1027)  loss_giou_dn_1: 1.4077 (1.4077)  loss_vfl_dn_2: 0.6807 (0.6807)  loss_bbox_dn_2: 0.1027 (0.1027)  loss_giou_dn_2: 1.4077 (1.4077)  loss_vfl_dn_3: 0.6481 (0.6481)  loss_bbox_dn_3: 0.1027 (0.1027)  loss_giou_dn_3: 1.4077 (1.4077)  loss_vfl_dn_4: 0.7239 (0.7239)  loss_bbox_dn_4: 0.1027 (0.1027)  loss_giou_dn_4: 1.4077 (1.4077)  loss_vfl_dn_5: 0.7493 (0.7493)  loss_bbox_dn_5: 0.1027 (0.1027)  loss_giou_dn_5: 1.4077 (1.4077)  time: 3.1752  data: 0.9293  max mem: 3409
Epoch: [0]  [100/315]  eta: 0:03:18  lr: 0.000010  loss: 26.6002 (29.0846)  loss_vfl: 0.5407 (0.4796)  loss_bbox: 0.3727 (0.5189)  loss_giou: 1.2841 (1.4501)  loss_vfl_aux_0: 0.4257 (0.3706)  loss_bbox_aux_0: 0.3710 (0.5383)  loss_giou_aux_0: 1.2907 (1.4848)  loss_vfl_aux_1: 0.4207 (0.3849)  loss_bbox_aux_1: 0.3715 (0.5382)  loss_giou_aux_1: 1.3563 (1.4772)  loss_vfl_aux_2: 0.3946 (0.3976)  loss_bbox_aux_2: 0.3682 (0.5284)  loss_giou_aux_2: 1.3447 (1.4720)  loss_vfl_aux_3: 0.4244 (0.4287)  loss_bbox_aux_3: 0.3794 (0.5265)  loss_giou_aux_3: 1.3208 (1.4646)  loss_vfl_aux_4: 0.4419 (0.4629)  loss_bbox_aux_4: 0.3677 (0.5207)  loss_giou_aux_4: 1.2987 (1.4554)  loss_vfl_aux_5: 0.4131 (0.3487)  loss_bbox_aux_5: 0.3844 (0.5667)  loss_giou_aux_5: 1.2959 (1.5055)  loss_vfl_dn_0: 0.3135 (0.3437)  loss_bbox_dn_0: 0.2471 (0.3055)  loss_giou_dn_0: 1.3106 (1.3656)  loss_vfl_dn_1: 0.3242 (0.3274)  loss_bbox_dn_1: 0.2422 (0.3021)  loss_giou_dn_1: 1.3100 (1.3735)  loss_vfl_dn_2: 0.3199 (0.3207)  loss_bbox_dn_2: 0.2418 (0.3005)  loss_giou_dn_2: 1.3243 (1.3899)  loss_vfl_dn_3: 0.3127 (0.3182)  loss_bbox_dn_3: 0.2404 (0.2994)  loss_giou_dn_3: 1.3255 (1.4081)  loss_vfl_dn_4: 0.3381 (0.3262)  loss_bbox_dn_4: 0.2362 (0.2985)  loss_giou_dn_4: 1.3058 (1.4245)  loss_vfl_dn_5: 0.3432 (0.3221)  loss_bbox_dn_5: 0.2330 (0.2981)  loss_giou_dn_5: 1.2965 (1.4407)  time: 0.9791  data: 0.0355  max mem: 5598
Epoch: [0]  [200/315]  eta: 0:01:44  lr: 0.000010  loss: 26.5438 (28.1184)  loss_vfl: 0.6016 (0.5424)  loss_bbox: 0.2409 (0.4421)  loss_giou: 0.9227 (1.3858)  loss_vfl_aux_0: 0.4387 (0.3864)  loss_bbox_aux_0: 0.3192 (0.4830)  loss_giou_aux_0: 1.1373 (1.4361)  loss_vfl_aux_1: 0.4278 (0.4231)  loss_bbox_aux_1: 0.2700 (0.4721)  loss_giou_aux_1: 1.0272 (1.4239)  loss_vfl_aux_2: 0.5253 (0.4678)  loss_bbox_aux_2: 0.2738 (0.4585)  loss_giou_aux_2: 0.9759 (1.4103)  loss_vfl_aux_3: 0.5477 (0.4903)  loss_bbox_aux_3: 0.2558 (0.4538)  loss_giou_aux_3: 0.9620 (1.4012)  loss_vfl_aux_4: 0.5385 (0.5201)  loss_bbox_aux_4: 0.2453 (0.4459)  loss_giou_aux_4: 0.9249 (1.3921)  loss_vfl_aux_5: 0.3764 (0.3502)  loss_bbox_aux_5: 0.3550 (0.5104)  loss_giou_aux_5: 1.1902 (1.4693)  loss_vfl_dn_0: 0.3414 (0.3371)  loss_bbox_dn_0: 0.2376 (0.2893)  loss_giou_dn_0: 1.2446 (1.3358)  loss_vfl_dn_1: 0.3576 (0.3325)  loss_bbox_dn_1: 0.2161 (0.2819)  loss_giou_dn_1: 1.1672 (1.3350)  loss_vfl_dn_2: 0.3676 (0.3314)  loss_bbox_dn_2: 0.1988 (0.2776)  loss_giou_dn_2: 1.1549 (1.3412)  loss_vfl_dn_3: 0.3746 (0.3305)  loss_bbox_dn_3: 0.1890 (0.2744)  loss_giou_dn_3: 1.1333 (1.3487)  loss_vfl_dn_4: 0.3864 (0.3372)  loss_bbox_dn_4: 0.1850 (0.2720)  loss_giou_dn_4: 1.1134 (1.3554)  loss_vfl_dn_5: 0.3814 (0.3367)  loss_bbox_dn_5: 0.1823 (0.2710)  loss_giou_dn_5: 1.1078 (1.3663)  time: 0.8950  data: 0.0281  max mem: 7279
Epoch: [0]  [300/315]  eta: 0:00:13  lr: 0.000010  loss: 24.7701 (27.2156)  loss_vfl: 0.4919 (0.5838)  loss_bbox: 0.1979 (0.3757)  loss_giou: 1.1908 (1.3334)  loss_vfl_aux_0: 0.4359 (0.4023)  loss_bbox_aux_0: 0.2100 (0.4350)  loss_giou_aux_0: 1.2578 (1.3978)  loss_vfl_aux_1: 0.4549 (0.4465)  loss_bbox_aux_1: 0.2088 (0.4119)  loss_giou_aux_1: 1.1868 (1.3771)  loss_vfl_aux_2: 0.4291 (0.4933)  loss_bbox_aux_2: 0.2160 (0.3951)  loss_giou_aux_2: 1.2271 (1.3610)  loss_vfl_aux_3: 0.4245 (0.5212)  loss_bbox_aux_3: 0.2070 (0.3882)  loss_giou_aux_3: 1.2386 (1.3504)  loss_vfl_aux_4: 0.4590 (0.5542)  loss_bbox_aux_4: 0.2092 (0.3802)  loss_giou_aux_4: 1.1867 (1.3412)  loss_vfl_aux_5: 0.3436 (0.3618)  loss_bbox_aux_5: 0.2588 (0.4701)  loss_giou_aux_5: 1.3033 (1.4427)  loss_vfl_dn_0: 0.3544 (0.3367)  loss_bbox_dn_0: 0.1631 (0.2777)  loss_giou_dn_0: 1.2213 (1.3142)  loss_vfl_dn_1: 0.3751 (0.3392)  loss_bbox_dn_1: 0.1509 (0.2652)  loss_giou_dn_1: 1.1694 (1.2945)  loss_vfl_dn_2: 0.3745 (0.3414)  loss_bbox_dn_2: 0.1474 (0.2576)  loss_giou_dn_2: 1.1288 (1.2881)  loss_vfl_dn_3: 0.3789 (0.3438)  loss_bbox_dn_3: 0.1480 (0.2535)  loss_giou_dn_3: 1.1069 (1.2887)  loss_vfl_dn_4: 0.3957 (0.3511)  loss_bbox_dn_4: 0.1495 (0.2506)  loss_giou_dn_4: 1.0977 (1.2907)  loss_vfl_dn_5: 0.3811 (0.3515)  loss_bbox_dn_5: 0.1504 (0.2494)  loss_giou_dn_5: 1.0994 (1.2988)  time: 0.9000  data: 0.0451  max mem: 7279
Epoch: [0]  [314/315]  eta: 0:00:00  lr: 0.000010  loss: 24.7760 (27.0520)  loss_vfl: 0.5932 (0.5909)  loss_bbox: 0.1958 (0.3679)  loss_giou: 1.1044 (1.3223)  loss_vfl_aux_0: 0.4245 (0.4032)  loss_bbox_aux_0: 0.2305 (0.4256)  loss_giou_aux_0: 1.1228 (1.3852)  loss_vfl_aux_1: 0.4470 (0.4489)  loss_bbox_aux_1: 0.2281 (0.4029)  loss_giou_aux_1: 1.1294 (1.3654)  loss_vfl_aux_2: 0.4555 (0.4961)  loss_bbox_aux_2: 0.2135 (0.3866)  loss_giou_aux_2: 1.1431 (1.3501)  loss_vfl_aux_3: 0.5099 (0.5277)  loss_bbox_aux_3: 0.1899 (0.3800)  loss_giou_aux_3: 1.1234 (1.3395)  loss_vfl_aux_4: 0.5212 (0.5625)  loss_bbox_aux_4: 0.1777 (0.3724)  loss_giou_aux_4: 1.1221 (1.3304)  loss_vfl_aux_5: 0.3762 (0.3631)  loss_bbox_aux_5: 0.2588 (0.4613)  loss_giou_aux_5: 1.2135 (1.4316)  loss_vfl_dn_0: 0.3486 (0.3369)  loss_bbox_dn_0: 0.1595 (0.2748)  loss_giou_dn_0: 1.2304 (1.3104)  loss_vfl_dn_1: 0.3751 (0.3402)  loss_bbox_dn_1: 0.1463 (0.2618)  loss_giou_dn_1: 1.1481 (1.2886)  loss_vfl_dn_2: 0.3745 (0.3425)  loss_bbox_dn_2: 0.1373 (0.2541)  loss_giou_dn_2: 1.1018 (1.2813)  loss_vfl_dn_3: 0.3739 (0.3450)  loss_bbox_dn_3: 0.1322 (0.2499)  loss_giou_dn_3: 1.0824 (1.2813)  loss_vfl_dn_4: 0.3787 (0.3522)  loss_bbox_dn_4: 0.1325 (0.2469)  loss_giou_dn_4: 1.0775 (1.2830)  loss_vfl_dn_5: 0.3811 (0.3527)  loss_bbox_dn_5: 0.1317 (0.2457)  loss_giou_dn_5: 1.0805 (1.2910)  time: 0.8673  data: 0.0341  max mem: 10329
Epoch: [0] Total time: 0:04:40 (0.8902 s / it)
Averaged stats: lr: 0.000010  loss: 24.7760 (27.0520)  loss_vfl: 0.5932 (0.5909)  loss_bbox: 0.1958 (0.3679)  loss_giou: 1.1044 (1.3223)  loss_vfl_aux_0: 0.4245 (0.4032)  loss_bbox_aux_0: 0.2305 (0.4256)  loss_giou_aux_0: 1.1228 (1.3852)  loss_vfl_aux_1: 0.4470 (0.4489)  loss_bbox_aux_1: 0.2281 (0.4029)  loss_giou_aux_1: 1.1294 (1.3654)  loss_vfl_aux_2: 0.4555 (0.4961)  loss_bbox_aux_2: 0.2135 (0.3866)  loss_giou_aux_2: 1.1431 (1.3501)  loss_vfl_aux_3: 0.5099 (0.5277)  loss_bbox_aux_3: 0.1899 (0.3800)  loss_giou_aux_3: 1.1234 (1.3395)  loss_vfl_aux_4: 0.5212 (0.5625)  loss_bbox_aux_4: 0.1777 (0.3724)  loss_giou_aux_4: 1.1221 (1.3304)  loss_vfl_aux_5: 0.3762 (0.3631)  loss_bbox_aux_5: 0.2588 (0.4613)  loss_giou_aux_5: 1.2135 (1.4316)  loss_vfl_dn_0: 0.3486 (0.3369)  loss_bbox_dn_0: 0.1595 (0.2748)  loss_giou_dn_0: 1.2304 (1.3104)  loss_vfl_dn_1: 0.3751 (0.3402)  loss_bbox_dn_1: 0.1463 (0.2618)  loss_giou_dn_1: 1.1481 (1.2886)  loss_vfl_dn_2: 0.3745 (0.3425)  loss_bbox_dn_2: 0.1373 (0.2541)  loss_giou_dn_2: 1.1018 (1.2813)  loss_vfl_dn_3: 0.3739 (0.3450)  loss_bbox_dn_3: 0.1322 (0.2499)  loss_giou_dn_3: 1.0824 (1.2813)  loss_vfl_dn_4: 0.3787 (0.3522)  loss_bbox_dn_4: 0.1325 (0.2469)  loss_giou_dn_4: 1.0775 (1.2830)  loss_vfl_dn_5: 0.3811 (0.3527)  loss_bbox_dn_5: 0.1317 (0.2457)  loss_giou_dn_5: 1.0805 (1.2910)
Test:  [ 0/75]  eta: 0:01:40    time: 1.3360  data: 0.7059  max mem: 10329
Test:  [10/75]  eta: 0:00:37    time: 0.5726  data: 0.0850  max mem: 10329
Test:  [20/75]  eta: 0:00:25    time: 0.4135  data: 0.0223  max mem: 10329
Test:  [30/75]  eta: 0:00:21    time: 0.4096  data: 0.0213  max mem: 10329
Test:  [40/75]  eta: 0:00:16    time: 0.4575  data: 0.0251  max mem: 10329
Test:  [50/75]  eta: 0:00:10    time: 0.3554  data: 0.0247  max mem: 10329
Test:  [60/75]  eta: 0:00:06    time: 0.2937  data: 0.0207  max mem: 10329
Test:  [70/75]  eta: 0:00:01    time: 0.3121  data: 0.0224  max mem: 10329
Test:  [74/75]  eta: 0:00:00    time: 0.3228  data: 0.0225  max mem: 10329
Test: Total time: 0:00:29 (0.3919 s / it)
Averaged stats: 
Accumulating evaluation results...
DONE (t=0.38s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.026
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.075
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.011
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.019
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.088
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.053
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.018
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.067
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.067
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.410
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.492
best_stat:  {'epoch': 0, 'coco_eval_bbox': 0.02619628208404876}